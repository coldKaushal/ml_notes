machine learning refers to a system capabale of the autonomous acquisition and integration of knowledge

need for ml:
	no human experts
	black box human expertise
	rapidly changing phenomena
	need for customisation

data: unprocessed fact
information: interpreted data, meaningful inference for the user
knowledge: Combination of inferred information, experiences, learning and insights.

type of data: training, valdation, testing

challenge in ml:
	avoid overlearing:
		holdout, cross validn, rgularisation, early stopping
	curse of dimensionality

	feature selection adn reduction
		selection:
			select feature
			remove that with missing val
			"	"	"	" 	" " low variance
			remove highly correlated
		reduction:
			pca
	tuning hyperparameters


ml types:
	supervised: bayesian, dt, linear regression
	unsupervised: clustering, pca, association rule
	reinforcement: q learning, monte carlo method




overfitting:
	def bc yeh likhni hai ab

	Given a model space H, a specific model h∈H is said to overfit the training data if there exists some alternative model h’∈H, such that h has smaller error than h’ over the training examples, but h’ has smaller error than h over the entire distribution of instances


roc:
	receiver operating characteristics
	tpr vs fpr
	in roc curve: area under 0.5 is useless
	auc: area under curve



decision tree:
	inductive learning tas
		use particular facts to make more generalised conclusion
	predictive model

	ID3
		all element in training set belong to same class then return the class

		if attribute is empty then return the max class

		else if training set is empty then return the leaf with default majority class

		else wahi kro jo tumhe pata hai

	entropy:
		minimum: 0
		max: log2(C) (C = no. of target class)
		gain tends to prefer splits that result in large no. of partition each being small but pure

		GainRation -> higher entropy partitioning in penalised

		GINI: min = 0, max = 1-1/c

	overfitting:
		tree growth went too far
		no. of instances belonging to node gets smaller as we build the tree

	in this case training error is not a good measure

	adv:
		inexpensive to construct
		fast in classfying
		easy to interpret in small trees
		good accuracy

	disav:
		axis parallel decision boundary
		redundancy
		need data to fit in memory to train the model
		need to retrain new data

	preprune:
		top down approach
		stop before full growth
		typical condn:
			all instance belong to same class
			all attribute value are same
		extra:
			#instance less than specified value
			class distribution is independent of available features(use chi square test)
			expanding current node doesnt improve impurity measure(GINI/GAIN)


	postprune:
		bottom up approach
		split data into train and test
		create full tree on train
		while accuracy on the test set increases:
			evaluate pruning each subtree-> replace by leaf-> majority vote
			replace subtree with leaf that increases the accuracy most

KNN
	instance based
	lazy learner
	non parametric(no assumption is made)
	eucledian distance is used usually
	k-> small = sensitive to noisy data
	k-> large = majority vote from other classes
	k < sqrt(n) (rule of thumb)

	we can normalise data and then use weighted distance measure if need to give priority to the feature
	this weight -> found from cross validation

	pros:
		simple and intuitive
		applied to any distribution
		good accuracy is large samples

	cons:
		more time
		choosing k is tricky
		need large samples for good accuracy

kmeans:
	unsupervised
	baaki algo pata hi hai
	use case:
		academic performance
		search engines
		diagnostic system
		wireless sensor network

	pros:
		simple
		flexible
		efficient
		suitable in large dataset

	cons:
		choosing k can be tricky
		sensitivity to noise and outlier
		sensitivity to scale
		only for numerical data



ensemble methods
	minimise variance:
		bagging
		random forest
	minimise bias:
		boosting-adaboost
		ensemble selection-choice of classifier

different classifier variety
	different population
	different feature
	different odelling technique
	different initial seeds

bagging
	bootstrap aggregation
	goal: reduce variance
	D = new sample with same size with random sampling
	take majority vote
	variance reduce sub linearly
	bias increases slightly
	use 1 type of classifier
	simple DT are mostly used
	easy to parallelise
	0.632 unique samples

Random forest
	reduce variance
	sample on data set and feature
	train DT
	voting
	sample size same
	sampling with replacement over instance
	sampling without replacement over attributes

boosting:
	no bootstrap sampling
	all data sample used sequntially
	next tree derives from previous tree

adaboost:
	adaptive boosting
	each time entire data is used
	training is sequential
	results in long tree unlike bagging/rf
	start with equal weight
	different weight after iteration
	final decision: weighted combination of the predicted output at each stage
	in aggregation with multiple the classifier with the importance factor of that classifier
	reduce bias i.e. optimise training
	can reduce variance if DT is pruned after training

naive bayes classifier
	based on bayes theorem
	easy to build
	usefull for large data sets

improve naive bayes model:
	if continous feature do not have normal distrbution:
		convert using transformation
	if test data has zero frequeny issue
		laplace correction
	remove correlated feature
	limited option for parameter tuning
	can't be ensembled(no variance to reduce)


bayesian network:
	directed acyclic graph
	node-> random varaible
	arrow from x to y means x is parent of y
	need conditional probability and dag to calcyulate probability in bbn
	parameters are probability in cpt






SVM:
	support vector machine
	supervised
	dimension-> feature
	non probabilistic linear classifier
	regularisation parameter: tells the SVM optimization how much you want to avoid misclassifying each training example.
	support vector: point closest to hyperplane, most difficult to classify, affects the hyperplane

	kernelling: mapping of data into higher dimension
	tuning parameter:
		margin
		regularisation
			factor: c
			c->small: underfit
			c->large-> overfit
		gamma
			define how far influences the calculation of plausible line
			low gamma: points far from line used in calc
			high gamma: points close to line used in calc  
		kerneling
			mathematical function for transforming data using some linear algebra
			ex:	linear kernel
				non linear kerne
				rbf(radial basis function)
				sigmoid
				polynomial
				exponential
			K(x, y) = <f(x), f(y)> = f(x).f(y)  (dot product)

	pros:
		well for clear marginal separation
		effective in high dimensional plane
		effective when dimension> sample
		memry efficient

	cons:
		large data set: high time
		noise-> not efficient
		










ANN
	artificial neural network
	simplistic imitation of a brain composed of dense net of simple structures
	typically 3 or more layers:
		input, hidden, output(can have multiple hidden layers)

	perceptron:
		simplified artificial neuron
		single layer neuron
		multiple inputs -> activation function ->output
	a bias 'b' is inroduced in equation which tells how easy is it to fire a perceptron

	backpropogation:
		calculate the gradient of cost function in a neural network

		used by gradient descent optimisation algo to update weights

	different activation functions:
		tanh(-1, 1)
		sigmoid(0,1)
		reLU(0, infini)

	application:
		object recognition
		object classificiation
		language translation
		speech recognition

	2 layer ann
		input -> hidden -> output

backpropogation:
	short for backward propogation of errors
	error = 1/2(prediction-actual)^2
	(1/2 is mathematical convenience)
	done using gradient descent

rnn:
	recurrent neural network
	good at modelling sequence data
	model:

input -> hidden -> output
          ^  |
          |__|

    causes short term memory because of vanishing gradient problem
    the earlier stage contribute least in the final output and also learns the least
    to tackle this lstm, gru were proposed




cnn
	convolution neural network
	feed forward neural network
	also called ConvNet
	broad layers:
		input layer: accepts pixels of image in form of array

		hidden layer: feature extraction

		output layer: 
			fully connected layer that identified the object in image
	hidden layer:
		1.) convolution layer
		2.) ReLU layer
		3.) Pooling Layer

	convolution layer:
		uses matrix filter and performs convolution operation to detect patterns in the image

	ReLU layer:
		once the feature maps are extracted the next step is this

		operations:
			performs element wise operation
			sets all negative pixel to 0
			introduces non linearity to network
			output is rectified feature map

	pooling layer:
		rectified feature map goes thru this.
		its a down sampling operation that reduces the dimension of the feature map

		ex: take a 2x2 matrix in the feature matrix and assign a 1x1 cell which is max value of it

		uses different flters to identify different parts of the image like edges, corners beaks of a bird


	there are multiple convolution, relu, pooling connected one after another that carry out feature extraction in every layer. input image is scanned multiple times to denerate input feature map

	flattening:
		process of converting resultant 2 D array from pooled feature map into 1D array

	fully connected -> flattened matrix from the pooling layer is fed as input to fully connected layer to classify the image










in rnn tanh function is used to squish the value between -1, 1

LSTM: long short term memory
GRU: Gated Recurrents units
	problem with rnn:
		RNN suffer from short term memory
		vanishing gradient
		earlier layers stop learning practically
	lstm:
		same control flow as rnn
		





     1     2

0    3     0

1    2     4


2/5, 4/4


3/5   1/6

err = irredu + b2 + c*v

